# model_pipeline.py
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os
import mlflow
import mlflow.sklearn
import psutil
import datetime
import time
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from mlflow import MlflowClient
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
)
from sklearn.model_selection import train_test_split
import logging
from mlflow import log_metric, log_param
from sklearn.metrics import classification_report, accuracy_score
import base64


def collect_system_metrics():
    """Collect system metrics for logging."""
    metrics = {}
    metrics["cpu_percent"] = psutil.cpu_percent(interval=1)
    memory_info = psutil.virtual_memory()
    metrics["memory_percent"] = memory_info.percent

    if psutil.sensors_temperatures():
        temps = psutil.sensors_temperatures()
        for name, entries in temps.items():
            for entry in entries:
                metrics[f"{name}_{entry.label}_temp"] = entry.current
    return metrics


def log_to_elasticsearch(metrics: dict, model_name: str, dataset: str):
    """Log metrics to Elasticsearch."""
    system_metrics = collect_system_metrics()

    # Log model metrics
    for key, value in metrics.items():
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "metric": key,
            "value": value,
            "model": model_name,
            "dataset": dataset,
        }
        try:
            es.index(index="mlflow-metrics", body=log_entry)
        except exceptions.ConnectionError as e:
            print(f"Connection error to Elasticsearch: {e}")
        except exceptions.RequestError as e:
            print(f"Request error in Elasticsearch: {e}")

    # Log system metrics
    for key, value in system_metrics.items():
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "metric": key,
            "value": value,
            "model": model_name,
            "dataset": dataset,
            "system_metric": True,
        }
        try:
            es.index(index="mlflow-metrics", body=log_entry)
        except exceptions.ConnectionError as e:
            print(f"Connection error to Elasticsearch: {e}")
        except exceptions.RequestError as e:
            print(f"Request error in Elasticsearch: {e}")


def prepare_data(path):
    """
    Charger et pr√©traiter les donn√©es :
    - Lecture du CSV
    - Encodage des variables cat√©gorielles
    - Affichage info, description et heatmap
    - S√©paration X / y + train_test_split
    """
    print("üì• Chargement des donn√©es...")
    data = pd.read_csv(path)
    print("‚úÖ Donn√©es charg√©es.")
    print("\nüìù V√©rification des valeurs nulles :")
    print(data.isnull().sum())
    print("\nüìù V√©rification des types de donn√©es :")
    print(data.dtypes)

    print("\nüìù V√©rification des doublons :")
    print(data.duplicated().sum())

    print("\nüìä Aper√ßu des donn√©es :")
    print(data.head())

    print("\nüìè Dimensions du dataset :", data.shape)

    print("\n‚ÑπÔ∏è Informations g√©n√©rales :")
    print(data.info())

    print("\nüìà Statistiques descriptives :")
    print(data.describe())

    print("\nüõ†Ô∏è V√©rification apr√®s pr√©paration :")
    print(data.head())
    print(f"Dimensions apr√®s pr√©paration : {data.shape}")

    # üîÑ V√©rifier si le dossier existe, sinon le cr√©er
    if not os.path.exists("static/images"):
        os.makedirs("static/images")

    # üîÑ Affichage de la heatmap et sauvegarde dans le bon dossier
    print("\nüî• Heatmap des corr√©lations :")
    plt.figure(figsize=(12, 6))
    sns.heatmap(
        data.select_dtypes(include=["float64", "int64"]).corr(),
        annot=True,
        cmap="coolwarm",
    )
    plt.title("Heatmap avant encodage")
    plt.savefig("static/images/correlation_heatmap.png")  # üîπ Nouveau chemin
    plt.close()
    print("‚úÖ Heatmap sauvegard√©e sous 'static/images/correlation_heatmap.png'")

    # Encodage des variables cat√©gorielles
    print("\nüî† Encodage des variables cat√©gorielles...")
    encoder = LabelEncoder()
    for col in data.select_dtypes(include="object").columns:
        data[col] = encoder.fit_transform(data[col])
    print("‚úÖ Encodage termin√©.")

    X = data.drop("Churn", axis=1)
    y = data["Churn"]
    return train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)


def log_visualization_to_elasticsearch(image_path, model_name, dataset):
    """Log image visualization to Elasticsearch."""
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode("utf-8")
        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "model": model_name,
            "dataset": dataset,
            "image": encoded_string,
            "image_type": image_path.split(".")[-1],
        }
        try:
            es.index(index="mlflow-visualizations", body=log_entry)
        except exceptions.ConnectionError as e:
            print(f"Connection error to Elasticsearch: {e}")
        except exceptions.RequestError as e:
            print(f"Request error in Elasticsearch: {e}")


def log_system_metrics():
    """Log system metrics to MLFlow during training."""
    cpu_percent = psutil.cpu_percent(interval=1)
    memory_info = psutil.virtual_memory()

    # Utilisation de noms de m√©triques simples
    mlflow.log_metric("CPU_Usage_Percentage", cpu_percent)  # Remplacement effectu√© ici
    mlflow.log_metric("Memory_Usage_Percentage", memory_info.percent)  # Et ici

    # V√©rification des temp√©ratures et enregistrement des donn√©es
    if psutil.sensors_temperatures():
        temps = psutil.sensors_temperatures()
        for name, entries in temps.items():
            for entry in entries:
                # Remplacer les espaces dans les noms des m√©triques
                metric_name = f"{name}_{entry.label}".replace(" ", "_")
                mlflow.log_metric(metric_name, entry.current)


def plot_confusion_matrix(y_true, y_pred, labels):
    """Generate and log confusion matrix plot."""
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels
    )
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.savefig("confusion_matrix.png")
    mlflow.log_artifact("confusion_matrix.png")
    plt.close()


def plot_roc_curve(y_true, y_pred):
    """Generate and log ROC curve plot."""
    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f"ROC curve (area = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], "k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Receiver Operating Characteristic (ROC)")
    plt.legend(loc="lower right")
    plt.savefig("roc_curve.png")
    mlflow.log_artifact("roc_curve.png")
    plt.close()


def train_model(X_train, y_train):
    print("üí° Entra√Ænement du mod√®le...")

    # üìå D√©marrage de l'exp√©rience MLFlow
    with mlflow.start_run():
        start_time = time.time()

        # üìå Log des param√®tres
        mlflow.log_param("n_estimators", 100)
        mlflow.log_param("max_depth", 10)

        # üìå Entra√Ænement
        model = RandomForestClassifier(n_estimators=100, max_depth=10)
        model.fit(X_train, y_train)

        # üìå Log des m√©triques
        accuracy = model.score(X_train, y_train)
        mlflow.log_metric("train_accuracy", accuracy)

        # üîç Pr√©dictions sur le set de train pour les m√©triques
        y_pred = model.predict(X_train)

        # üìå Log des m√©triques avanc√©es
        report = classification_report(y_train, y_pred, output_dict=True)
        mlflow.log_metric("Precision", report["weighted avg"]["precision"])
        mlflow.log_metric("Recall", report["weighted avg"]["recall"])
        mlflow.log_metric("F1_Score", report["weighted avg"]["f1-score"])

        # ‚è≤Ô∏è Log du temps d'entra√Ænement (le nom est modifi√© pour √™tre accept√© par MLflow)
        end_time = time.time()
        mlflow.log_metric("Training_Time_seconds", end_time - start_time)

        # üîç Log des m√©triques syst√®me

        # üìä G√©n√©ration des artefacts
        plot_confusion_matrix(y_train, y_pred, labels=model.classes_)
        plot_roc_curve(y_train, y_pred)

        # üíæ Enregistrement du mod√®le
        mlflow.sklearn.log_model(model, "model")
        print("‚úÖ Mod√®le enregistr√© avec MLFlow.")
        # üìå Log du mod√®le versionn√© dans le registre MLflow
        client = MlflowClient()
        model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"

        # Ajouter √† un mod√®le registry
        model_name = "Churn_Prediction_Model"

        try:
            # Enregistrer la version du mod√®le
            version = client.create_model_version(
                model_name, model_uri, mlflow.active_run().info.run_id
            )

            # Ajouter des informations suppl√©mentaires (description)
            client.update_model_version(
                model_name,
                version.version,
                description="Churn prediction model using Random Forest",
            )

            # Ajouter des tags √† la version du mod√®le
            client.set_model_version_tag(
                model_name, version.version, "model_type", "RandomForest"
            )
            client.set_model_version_tag(
                model_name, version.version, "domain", "Churn Prediction"
            )
            print(f"‚úÖ Mod√®le versionn√© et enregistr√© : Version {version.version}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'enregistrement du mod√®le versionn√© : {e}")

    return model


def evaluate_model(model, X_test, y_test):
    """
    √âvalue le mod√®le √† l'aide de la pr√©cision et d'un rapport de classification.
    """
    print("üß™ √âvaluation du mod√®le...")
    predictions = model.predict(X_test)
    acc = accuracy_score(y_test, predictions)
    print(f"\nüéØ Accuracy: {acc:.4f}")
    print("\nüìã Rapport de classification :\n")
    print(classification_report(y_test, predictions))
    # Dans la fonction `evaluate_model`:
    generate_html_report(model, X_test, y_test)
    # Generate classification report and store it
    report = classification_report(y_test, predictions, output_dict=True)
    print(classification_report(y_test, predictions))

    # Prepare metrics for Elasticsearch
    # Prepare metrics for Elasticsearch
    metrics = {
        "test_accuracy": acc,
        "test_precision": report["weighted avg"]["precision"],
        "test_recall": report["weighted avg"]["recall"],
        "test_f1_score": report["weighted avg"]["f1-score"],
        "roc_auc": auc(*roc_curve(y_test, model.predict_proba(X_test)[:, 1])[:2]),
    }

    # Confusion Matrix
    cm = confusion_matrix(y_test, predictions)
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=["Non-Churn", "Churn"],
        yticklabels=["Non-Churn", "Churn"],
    )
    plt.xlabel("Pr√©dictions")
    plt.ylabel("R√©el")
    plt.title("Matrice de confusion")
    plt.savefig("confusion_matrix.png")
    mlflow.log_artifact("confusion_matrix.png")
    plt.close()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f"ROC curve (area = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], "k--")
    plt.xlabel("Taux de faux positifs")
    plt.ylabel("Taux de vrais positifs")
    plt.title("Courbe ROC")
    plt.legend(loc="lower right")
    plt.savefig("roc_curve.png")
    mlflow.log_artifact("roc_curve.png")
    plt.close()

    print("‚úÖ Visualisations sauvegard√©es : confusion_matrix.png, roc_curve.png")


def save_model(model, filename="model.joblib"):
    """
    Sauvegarde le mod√®le entra√Æn√© avec joblib.
    """
    joblib.dump(model, filename)
    print(f"üíæ Mod√®le sauvegard√© sous '{filename}'.")


def load_model(filename="model.joblib"):
    """
    Charge un mod√®le sauvegard√© avec joblib.
    """
    print(f"üìÇ Chargement du mod√®le depuis '{filename}'...")
    return joblib.load(filename)


def save_data(X_train, X_test, y_train, y_test, filename="data_split.pkl"):
    """
    Sauvegarde les datasets d'entra√Ænement et de test.
    """
    with open(filename, "wb") as f:
        joblib.dump((X_train, X_test, y_train, y_test), f)
    print(f"üíæ Donn√©es sauvegard√©es sous '{filename}'.")


def load_data(filename="data_split.pkl"):
    """
    Charge les datasets d'entra√Ænement et de test.
    """
    print(f"üìÇ Chargement des donn√©es depuis '{filename}'...")
    with open(filename, "rb") as f:
        return joblib.load(f)


def generate_html_report(model, X_test, y_test, filename="model_report.html"):
    """
    G√©n√®re un rapport HTML d√©taill√© avec les m√©triques, les courbes ROC,
    la matrice de confusion et l'importance des features.
    """
    print("üìä G√©n√©ration du rapport HTML...")

    # Pr√©dictions
    y_pred = model.predict(X_test)

    # Rapport de classification
    report = classification_report(y_test, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose()

    # Matrice de confusion
    conf_matrix = confusion_matrix(y_test, y_pred)

    # Feature Importances
    if hasattr(model, "feature_importances_"):
        feature_importance = pd.DataFrame(
            {"Feature": X_test.columns, "Importance": model.feature_importances_}
        ).sort_values(by="Importance", ascending=False)
    else:
        feature_importance = pd.DataFrame()

    # Courbe ROC
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)

    # Cr√©ation des graphiques
    if not os.path.exists("reports"):
        os.makedirs("reports")

    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
    plt.title("Matrice de confusion")
    plt.savefig("reports/confusion_matrix.png")
    plt.close()

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
    plt.plot([0, 1], [0, 1], linestyle="--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Courbe ROC")
    plt.legend()
    plt.savefig("reports/roc_curve.png")
    plt.close()

    # G√©n√©ration du HTML
    with open(filename, "w") as file:
        file.write(f"<h1>Rapport de Mod√®le</h1>")
        file.write("<h2>M√©triques de Classification</h2>")
        file.write(report_df.to_html())

        file.write("<h2>Importance des Features</h2>")
        if not feature_importance.empty:
            file.write(feature_importance.to_html())
        else:
            file.write("<p>Le mod√®le n'a pas d'importance de features.</p>")

        file.write("<h2>Matrice de Confusion</h2>")
        file.write('<img src="reports/confusion_matrix.png" width="600">')

        file.write("<h2>Courbe ROC</h2>")
        file.write('<img src="reports/roc_curve.png" width="600">')

    print(f"üìå Rapport g√©n√©r√© avec succ√®s : {filename}")


def log_system_metrics():
    """Log system metrics to MLFlow during training."""
    cpu_percent = psutil.cpu_percent(interval=1)
    memory_info = psutil.virtual_memory()
    mlflow.log_metric("CPU Usage (%)", cpu_percent)
    mlflow.log_metric("Memory Usage (%)", memory_info.percent)

    if psutil.sensors_temperatures():
        temps = psutil.sensors_temperatures()
        for name, entries in temps.items():
            for entry in entries:
                mlflow.log_metric(f"{name}_{entry.label}", entry.current)
